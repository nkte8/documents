---
title: ""
emoji: "🗄"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["zfs","kubernetes"]
published: true
---

ZFSを用いてストレージサーバ・を構築してみたので備忘録です。  

# 注意点

zfsでストレージを構築するにあたっては、**先にちゃんと構成を考えた方がいいです。**
増やすことは容易ですが、**減らすのは非常に難しい**

# 機器  

どうしても2.5インチのディスクでスモールにいきたかったので、2.5インチベイのHDDケースを揃えました。

- ラズベリーパイ
  - 8GBモデル
- 2.5インチHDD
  - 1.0 TB x 3
    - TOSHIBA 3台
  - 2.0 TB x 2
    - TOSHIBA 2台
- 3.5インチHDD
  - 2.0 TB x 1
    - IO-DATAの家庭用外付けHDD
- ハードディスクケース  
  - Vantec 2台用 2.5インチ HDD/SSDケース
    - ~~https://www.amazon.co.jp/gp/product/B07CSJ1PB2~~
    - アクセスが多いと不可解なディスクダウンが発生しました
      - 不良品かもしれないが、ACアダプタがUSB給電だったりと信頼性低い気がする
  - ラトックシステム USB3.2 Gen2 RAIDケース
    - https://www.amazon.co.jp/gp/product/B08F2B4HNG
    - こっちは問題なく動作。RAIDの機能は正直不要
      - ACアダプタも大きくないのでマシではある

# ZFSの導入  
ZFSはカーネルに組み込むことができる他、次の手順のようにモジュールを一般パッケージチックにインストールすることで使用可能になります。  

## 最新ではなくていい場合  
かなり古いみたいです（`0.8.3`?）ただ、ZFS内部のバージョンは`28`なので、特に問題はありません。
```sh
apt install -y zfsutils-linux
```

## ZFS最新版の導入

最新バージョンはgithubにリポジトリが存在します。   
https://github.com/zfsonlinux/zfs.git
以下を参考にビルドを実施します。   
https://qiita.com/yamakenjp/items/380ea5bb338940b5dc55  
```sh
sudo su -
# ビルドのみコンテナ内で実施する
mkdir ./work
docker run --rm -v ${PWD}/work:/root/ -it ubuntu:20.04 /bin/bash
# コンテナ内
cd ~
apt update
DEBIAN_FRONTEND=noninteractive apt -y install build-essential autoconf automake libtool gawk alien fakeroot dkms libblkid-dev uuid-dev libudev-dev libssl-dev zlib1g-dev libaio-dev libattr1-dev libelf-dev linux-headers-$(uname -r) python3 python3-dev python3-setuptools python3-cffi libffi-dev python3-packaging git gdebi-core
git clone https://github.com/zfsonlinux/zfs.git && cd zfs
git checkout zfs-2.0.4
./autogen.sh
./configure --enable-systemd
make -j1 deb-utils deb-dkms
exit
# コンテナ外
cd work/zfs
ls -la # *.debが生成されている
# ホストにgdebiをインストールする
sudo apt install -y gdebi-core linux-headers-$(uname -r)
for file in *.deb; do sudo gdebi -q --non-interactive $file; done
sudo systemctl enable zfs.target
# デーモンを手動で起動
sudo systemctl start zfs.target
# ZFS プールを再起動時に自動的にマウントするようサービス・ターゲットを有効にする
sudo systemctl enable zfs-import-cache
sudo systemctl enable zfs-mount
sudo systemctl enable zfs-import.target
# 再起動
reboot
```
再起動後、zpoolコマンドが使用可能になります。  
```log
root@ubuntu:~# zpool list
no pools available
root@ubuntu:~# zfs list
no datasets available
```

# ZFSの導入  

## ディスクの準備  
既存のファイルシステムを削除します（`wipefs -a <デバイス>`）
```log
root@ubuntu:~# wipefs -a /dev/sda
/dev/sda: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54
/dev/sda: 8 bytes were erased at offset 0x1d1c1115e00 (gpt): 45 46 49 20 50 41 52 54
/dev/sda: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa
/dev/sda: calling ioctl to re-read partition table: Success
root@ubuntu:~# wipefs -a /dev/sdb
/dev/sdb: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54
/dev/sdb: 8 bytes were erased at offset 0x1d1c1115e00 (gpt): 45 46 49 20 50 41 52 54
/dev/sdb: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa
/dev/sdb: calling ioctl to re-read partition table: Success
```

https://github.com/openzfs/zfs/issues/5032  
https://forums.unraid.net/topic/41333-zfs-plugin-for-unraid/page/19/  

## プールの作成

プールを作成します。HDDが4096セクタに対応している場合は、`ashift=12`を設定します。  
```sh
zpool create -o ashift=12 -d zfs /dev/sda1 /dev/sdb1
zpool upgrade -a
# 途中で停止するので再起動
reboot
# 再度upgadeを実施すると下完了する
zpool upgrade -a
```

```log
root@sandbox:~# zpool upgrade -a
This system supports ZFS pool feature flags.

Enabled the following features on 'zfs':
  async_destroy
  empty_bpobj
  lz4_compress
  multi_vdev_crash_dump
  spacemap_histogram
  enabled_txg
  hole_birth
  extensible_dataset
  embedded_data
  bookmarks
  filesystem_limits
  large_blocks
  large_dnode
  sha512
  skein
  edonr
  userobj_accounting
  encryption
  project_quota
  device_removal
  obsolete_counts
  zpool_checkpoint
  spacemap_v2
  allocation_classes
  resilver_defer
  bookmark_v2
cannot set property for 'zfs': invalid feature 'redaction_bookmarks'
root@sandbox:~# reboot
root@sandbox:~# zpool upgrade -a
This system supports ZFS pool feature flags.

Enabled the following features on 'zfs':
  redaction_bookmarks
  redacted_datasets
  bookmark_written
  log_spacemap
  livelist
  device_rebuild
  zstd_compress

```








zpool上にFSを作成したりできます。  
```sh
zfs create zfs/Data
zfs create zfs/recov
zfs set quota=0.9T zfs/recov
```


## 共有設定

今回はsambaを用いて共有しようと思います。  

既知の問題に対応(https://www.bunbun-etcetera.net/etcetera/2020/01/sambazfs.html)  
```sh
# casesensitivityはReadonlyのため、create時に設定が必要です。  
zfs create -o casesensitivity=insensitive zfs/Data
zfs set compression=lz4 atime=off acltype=posixacl zfs/Data
zfs set quota=0.9T zfs/Data
zfs set mountpoint=/Data zfs/Data
```
dockerを用いてセットアップをしてもよいのですが、ZFSノードなので安定性を考慮してホストに直接インストールします。  
<!-- https://github.com/dperson/samba/blob/master/Dockerfile -->

```
apt install -y samba

```



## 備考: ディスクの追加  

さらにディスクを購入し、`/dev/sda`と`/dev/sdb`を接続&`gdisk`でパーティションを作成
```log
root@sandbox:~# blkid
...
/dev/sdc1: LABEL="zfs" UUID="7284141825033654125" UUID_SUB="15355404550027129709" TYPE="zfs_member" PARTLABEL="Linux filesystem" PARTUUID="92f68941-a5ac-43e9-9976-b1bd6e463b0b"
/dev/sdd1: LABEL="zfs" UUID="7284141825033654125" UUID_SUB="16475449539013955587" TYPE="zfs_member" PARTLABEL="Linux filesystem" PARTUUID="e40ced58-9beb-4183-81f3-be6e42e1197e"
/dev/sda1: PARTLABEL="Linux filesystem" PARTUUID="1f31d349-0794-4602-aa6c-bfe8a45a1e9d"
/dev/sdb1: PARTLABEL="Linux filesystem" PARTUUID="d4ff8b81-7761-49e3-8960-595a53e0e4e3"
root@sandbox:~# zpool status zfs
  pool: zfs
 state: ONLINE
  scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	zfs         ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sdc1    ONLINE       0     0     0
	    sdd1    ONLINE       0     0     0

errors: No known data errors
```


ディスクの追加は慎重に（削除が難しいため）
```sh
# -n を外すと実際に実行される
zpool add zfs -n mirror /dev/sda1 /dev/sdb1 # 新たにmirror poolを追加する（拡張）
```
## 備考: 移行

上記の操作はsandboxノードで実行しました。これをinfraノードに移動できるか？
すごい簡単。  
```sh
# 以下を現行マシンで実行
zpool export <プール名>
# 以下を移行先マシンで実行 & 機能のアップグレード
zpool import <プール名>
zpool upgrade <プール名>
```

```log
root@ubuntu:~# zpool list
NAME   SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
zfs   1.81T   852K  1.81T        -         -     0%     0%  1.00x    ONLINE  -
root@ubuntu:~# zpool export zfs
root@ubuntu:~# zpool list
no pools available
```

```log
(infraノードにディスクの接続先を変更)
root@infra01:~# zpool list
no pools available
root@infra01:~# zpool import zfs
root@infra01:~# zpool list
NAME   SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
zfs   1.81T  1.17M  1.81T        -         -     0%     0%  1.00x    ONLINE  -
root@infra01:~# zpool status
  pool: zfs
 state: ONLINE
  scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	zfs         ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    sdb1    ONLINE       0     0     0
	    sdc1    ONLINE       0     0     0

errors: No known data errors
root@infra01:~# zpool upgrade zfs
This system supports ZFS pool feature flags.

Pool 'zfs' already has all supported features enabled.
```


# CSIの導入







# 備考

https://yoshida-eth0.hatenablog.com/entry/20120717/1342470808  
https://blog.programster.org/zfs-create-disk-pools  
https://wiki.archlinux.jp/index.php/ZFS
移行について  
https://rabbit-note.com/2020/06/02/zfs-export-import/
Solaris 11.4では追加したmirrorも取り外しが可能な様子
https://shakemid.hatenablog.com/entry/2019/02/06/222913
Samba-csiがarm64に対応していないため、使えない。
https://github.com/kubernetes-csi/csi-driver-nfs/issues/199
`block size: 512B configured, 4096B native`の対策  
https://www.seichan.org/blog/2013/12/post-237.html