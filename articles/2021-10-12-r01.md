---
title: "k8sを用いた監視カメラシステムの構築"
emoji: "📹"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["iot","raspberrypi","kubernetes","ffmpeg","container"]
published: true
---

RaspberryPiのカメラモジュールを用いて監視カメラを構築するのは定石ですが、今回は裏方などでよく見る監視カメラの一元管理システムを開発してみました。　　

`監視カメラの一元管理システム`なんてかっこつけてますが、要は複数台のカメラの映像を一つのブラウザ画面上で閲覧できるやつです。↓みたいな...  
![](/images/2021-10-12-r01/screenshot.png)  
本記事では、ざっくりどう構成・実装したのかを説明していきたいと思います。

# 概要
## リポジトリ  
詳しいコードは以下をご覧ください。
https://github.com/nkte8/monitoring/tree/2021-11-28-r01

## 全体構成  
以下の2パターンのデバイスがあります。  
- エッジデバイス
    - 低スペック・複数台を想定
    - カメラデバイスと接続されている
    - 今回は RaspberryPi 3+ Model A (RAM 512MB) を利用 
- ストリーミングサーバ
    - クラウド環境を想定
    - 今回はkubernetesクラスタを利用
        - RaspberryPi 4 Model B (RAM 8GB) 6台で構成されている   

カメラからの映像は以下のような経路でクライアントに配信されます。
![](/images/2021-10-12-r01/dataflow.drawio.png)   

エッジデバイスからはデータを垂れ流すだけで、動画の処理や配信はクラウド上のサーバで行います。  
ちなみにエッジデバイスにはRaspberryPi 3Aを用いました。5GHz帯のWifiを利用したかったためです。

## コンポーネントについて   
どういったソフトウェアを使っていくのか記述していきます。　　
![](/images/2021-10-12-r01/ossinfo.drawio.png)   
今回は以下のコンポーネントを使用します。コアなものについては太字にしています。  
- **v4l2rtspserver**  
    - rtspサーバアプリケーション
        - https://github.com/mpromonet/v4l2rtspserver
    - rtspは遅延が非常に少ないストリーミング用プロトコル
    - raspios搭載のraspividのTCP配信より低遅延
- **opencv**
    - 高性能画像処理ライブラリ
        - https://opencv.org
    - rtspを直接受信し、フレーム単位で画像に処理を行う
- **ffmpeg**
    - 高性能動画処理アプリケーション
        - https://www.ffmpeg.org
    - opencvから出力されるフレーム情報をリアルタイムで連結し、HLSとして配信
    - 定期的にHLSとして作成された映像をmp4に変換
- nginx
    - OSSのWebサーバ
    - HLSの動画リスト(m3u8)を再生するクライアントとして使用

ざっくり流れをかくと、`v4l2rtspserver`から配信されるストリーミング（RTSP）を`opencv`でフレーム単位で取得、編集したフレームを止めどなく`ffmpeg`に入力として渡します。ffmpegはこれをHLS形式に変換し、変換されたファイルを`nginx`で閲覧するという具合です。  

## k8s上での展開方法について    
![](/images/2021-10-12-r01/onk8s.drawio.png)   
今回開発といった開発を行った項目を太字にしておきます。  
- rtspサーバー
    - docker-composeでv4l2rtspserverを起動
        - *kubernetes管理ではない*
        - RaspberryPi 3Aでkubeletが安定して動作しなかったため
- **rtsp2hls**
    - opencvでrtspを受信し標準出力、標準出力をffmpegでhlsに変換するスクリプト
    - deploymentで動作させる。configmapを用いてアクセスするrtspサーバを指定
    - 排他処理はプレイリストの有無で判断
        - rtspサーバへの接続台数が取得できたらよかった
- **hls2mpeg**
    - ffmpegでhlsをmp4で結合するスクリプト
    - cronjobで実施
- nginx
    - videojsを用いてhlsを再生
        - https://videojs.com
        - videoタグで囲むだけでm3u8をあらゆるブラウザで再生できるようになる

図に書きそびれたのですが、クライアントは`nginx`コンテナを`Service`の`ExternalIP`越しで閲覧できるということです。  

# 実装について  
今回開発した機能について軽く説明します。細かいコードはgithubで参照していただくとして、本記事では処理の流れなどをまとめておきます。  
## rtsp2hlsについて
rtspサーバからのストリーミングデータを、ブラウザがネイティブで対応しているストリーミング動画形式であるHLSフォーマットに、リアルタイム変換するスクリプトです。
### 構成
今回はDeploymentを用いて実装することを考えました。

**カメラ映像の受信は1台1プロセスという単位で処理**する必要があり、本来はカメラごとにPodを用意する必要があります。このためk8s上で実装するのであれば、Podを用いてyamlおよび設定ファイルをカメラ台数分作成しなければなりません。

Deploymentを持ちいれば、yamlファイルの数は1つで十分になります。ただし*DeploymentではレプリカはあくまでPodの複製のため、カメラごとの個別に設定ファイルを作成することはできません*。

このため、今回はカメラ全ての設定ファイルをアプリケーションで読み込み、アプリケーション内で排他処理を行い実現しました。

### 実際の実装
こちらはrtsp2hlsコンテナの中で実施している内容になります。
![](/images/2021-10-12-r01/rtsp2hls-detail.drawio.png)   
bashスクリプト内でpython（opencv）とffmpegを制御しています。rtspをpythonで受け、opencvにより加工を行った後、標準出力にフレームのデータを出力します。これをffmpegの入力として受け、入力の有る限り変換をし続けるという仕組みになっています。

### 理想の構成  
今回はアプリケーション側で対処を行いましたが、今回の対応はあまりk8sらしい実装とは言い難く、本来は次のように構成すべきです。  
![](/images/2021-10-12-r01/rtsp2hls-future.drawio.png)   
[構成](#構成)でも記載しましたが、各カメラごとに設定が異なり、プロセスも各カメラごとに設定ファイルを持つ必要があるため、k8sで実装するなら最小単位はPodです。  

ただ、Podはオートヒールしないこと、設定ファイルを書く量が増えることを考慮すると、Podを制御するcontrollerのようなサービスを実装し、controllerから直接kube-api-serverへデプロイをさせるのがスマートな構成だと考えています。  

## hls2mpegについて  
hlsのセグメントをmp4に変換するスクリプトになります。  
### 構成  
こちらはCronjobを用いて構築します。毎日、蓄積した動画に対して変換を実行するだけのため、デーモン化の必要がないためです。

HLS動画をmp4に変換するだけですが、プロセスが途中で停止してしまったり、動画変換が失敗してしまった時のことを考え、最低限の排他処理は実装しました（lockファイルなどの簡易なものです）
### 実際の実装  
コンテナが途中で終了されてしまった場合にもlockファイルは削除せず、再変換等は実施しません。
![](/images/2021-10-12-r01/hls2mpeg-detail.drawio.png)   
セグメントは一度全て結合したmp4にしてから変換をかけています、ファイル数が多いとIOの効率が悪いためです。また、結合しただけだとファイルサイズがかなり大きいため、画質を落として動画を24倍速（動画時間1時間）のアーカイブに変換しています。

### 実装の別解
こちらはrtsp2hlsとは異なり、設定ファイルなどは用いないため、純粋な排他制御とサブプロセスのマイクロサービス化が課題になってきます。
![](/images/2021-10-12-r01/hls2mpeg-future.drawio.png)   
特に、ffmpegをコンテナ内でサブプロセスとして複数回実行するのは処理単位としては大きめで、本来は全てのプロセスがコンテナとして分断するべきです。
とはいえ、分断しすぎてもわかりづらくなってしまうこともあるため、最適とも言えないところが難しい部分ではあります。  

# 今後の改善  
想定より開発に時間がかかり、いったん完成させようという方向で進めたため、細かいところで気にいっていないところ、いけてない部分など、改善点多いです。  
- Webインターフェースの改善
    - 現在はnginxにべたうちでm3u8を読み込むように記述している
        - configmap等をを読み込んでhtmlを動的に作成したい
    - hlsではなくrtspで直接接続できるようにしたい
        - hlsよりrtspの方が低遅延なため
- 構成の改善
    - [rtsp2hlsアプリケーションの理想の構成](#理想の構成)を参照
    - インフラとアプリケーションの分断によるマイクロサービス化
        - マイクロサービス化はチーム開発の際に真価を発揮する
        - 現状は個人開発のため、勉強目的でいじっていきたい
- コンテナアプリケーションの改善
    - サブプロセスをあまり使わないようにしたい
        - [hls2mpegアプリケーションの実装の別解](#実装の別解)などの検討
        - コンテナの思想が1プロセス1コンテナであるため
        - 拗らせすぎると逆に管理困難になるため、さじ加減が大事
    - bashではなくpython内でffmpegを利用する方法にしたい
        - ffmpegはC言語で書かれているため、最終的にはCでのincludeが理想  

# 備考  
プロジェクト初期は、MQTTを使うことやWebsocketで直接受信することなどを試しましたが、結局遅延が大きくなったり、処理が複雑になったりして、最終的にこの方法に落ち着きました。  
rtspでのストリーミング配信には多少マシンパワーを必要とすると考えていましたが、近年のラズベリーパイのスペックの向上は目を見張るものがあり、十分に動作を確認できたという次第です。
改善点が多いため、よりいっそう良いシステムにしていきたいです。

# 参考  

https://qiita.com/wktq/items/a6e169e85a8a75c8524f  
https://www.dpsj.co.jp/tech-articles/wowza-blog-hls  
https://did2memo.net/2017/02/20/http-live-streaming/
https://github.com/mpromonet/v4l2rtspserver
https://opencv.org
https://www.ffmpeg.org
https://videojs.com
