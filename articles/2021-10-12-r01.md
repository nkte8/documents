---
title: "MQTT+Opencv+HLS用いて監視カメラ開発した"
emoji: "📹"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["raspberrypi","docker","python","go","kubernetes"]
published: true
---

RaspberryPiのカメラモジュールを用いて、監視カメラ、およびストリーミングサーバを開発してみました。資材はgithubに配置したため、やってみたことや実装について説明していきたいと思います。

# 要件の整理

本システムについて説明していきます。

## 機材について  

- エッジデバイス
    - 低スペック・複数台を想定
    - カメラを装着している
    - 今回は RaspberryPi 3+ Model A (RAM 512MB) を利用 
- ストリーミングサーバ
    - クラウド環境を想定
    - 今回はkubernetesクラスタを利用
        - RaspberryPi 4 Model B (RAM 8GB) 6台で構成されている
    
カメラを装着したRaspberryPi（エッジデバイス）と、信号を受けて処理を行うRaspberryPi（ストリーミングサーバ）を分けていることがポイントです。エッジデバイスはあくまでカメラから得た情報をサーバに配信し、サーバ側で処理・配信を行います。

## データの送信方法について  

端末のデバイスで取得したデータの配信方法はいくつかあり、ライブ配信を考えた際にはそれぞれにメリット・デメリットがあります。  
ここでは`プロトコルの幅`・`データの加工難易度`・`データの転送量`の3点で列挙しました。

- 画像ファイル
    - コマ撮り画像としてデータを配信
        - 一般的なプロトコルが利用可能
    - 画像はデータの加工が容易・高速
        - その後の動画化などには負荷を要する
    - データの転送量は調節による
        - 画質・1秒毎の送信枚数
- 動画ファイル
    - 一定区間の録画としてデータを配信
        - 一般的なプロトコルが利用可能
    - 動画データは加工が困難・低速
        - 動画同士の連結などは低負荷
    - データの転送量は調節による
        - 画質・ビットレート・fpsなどの設定
- デバイスソケット
    - デバイス自体のサーバとして配信
        - ストリーミング用のプロトコルが必要
    - ストリーミングデータに対して加工は困難
        - 何らかの方法で受信したデータに対して加工が必要
    - データの転送量は調節不可

## 配信プロトコルについて

ストリーミング用のプロトコルを使う場合は以下の選択肢があります。  

- RTMP
    - Adobeによりflashベースで開発されていた（開発終了）
    - 配信データ: 動画ファイル
    - 遅延: 0.2〜30秒（バッファする動画の再生時間による）
    - 配信方法: flash playerや動画playerの埋め込みが必須
- RTP/RSTP
    - TCP/UDPを用いたストリーミングプロトコル
    - 配信データ: デバイスソケット
    - 遅延: 0.2〜5秒
    - 配信方法: 動画playerの埋め込みが必須
- MPEG-DASH
    - ISO国際標準規格。
    - 配信データ: 動画ファイル
    - 1〜45秒（バッファする動画の再生時間による）
    - 配信方法: 動画playerの埋め込みが必須
- HLS
    - Apple社が開発しているストリーミング規格
    - 配信データ: 動画ファイル
    - 1〜60秒（バッファする動画の再生時間による）
    - 配信方法: 一部のクライアントはHTML5の標準でサポート

RTP/RSTPはデバイスソケットを共有するので、遅延は少ないものの負荷は高そうです。他のプロトコルは動画ファイルを扱うため、どうしてもバッファの関係で必ず遅延が発生してしまいます。

一般的なプロトコルの場合は以下の方法があります。
- HTTP
    - 一般的な通信プロトコル
    - 同期処理・片方行通信
- FTP
    - ファイル転送用プロトコル・二ポート使用する
    - 同期処理・片方向通信
    - 二ポート使用するためHTTP通信より転送が早い
- Websocket
    - HTTPにおいてリアルタイムで双方向通信するために開発された
    - 非同期処理・双方向通信
    - 通信自体は一対一が想定される
- AMQP
    - メッセージングプロトコル、ブローカーを介して一対多の通信が可能
    - 非同期処理・双方向通信
    - 高性能・高信頼・OpenStackなどで使用される
- MQTT
    - IoTの実現のため開発された。ブローカーを介して一対多の通信が可能
    - 非同期処理・双方向通信
    - シンプル・低負荷・最低限の機能のみで構成

データをリアルタイムで取得するためには前提として非同期処理である必要があります。Websocket・AMQT・MQTTの選択肢が上がりますが、カメラが複数台あることを考えるとブローカー型が好ましいです。AMQTは高性能・高信頼ではありますが、リソースが制限される以上、動作が継続する保証は低いです。

以上のことから、今回はMQTTを用いることにしました。

## 構成について

以下のような構成・処理フローを考えました。  

![](/images/2021-10-12-r01/workflow-system.drawio.png)  

データを送信するだけのエッジデバイスと、データの受信・処理・配信を担うクラスターで構成されます。  
ストリーミングを行うサーバを複数設置、これをロードバランスすることで可用性を確保。後からデータを確認できるよう別途動画データをエンコードする機構も設置しておきます。  

こちらをk8s上で考えると次のような構成で実施できそうです。  
![](/images/2021-10-12-r01/workflow-k8s.drawio.png)  

HTTPサーバと接続しているMQTT Clientへのデータの送信に関しては、クライアントからリクエストがあったサーバのみで実施する仕組みを利用します。

<!--
## エッジデバイス→ストリーミングサーバの通信について

カメラデバイスで取得し配信できるデータについて、カメラから取得できるデータは以下の2種類があります。

- コマ撮り画像
    - メリット: 動画よりデータ通信量が減る・データを加工しやすい
    - デメリット: 動画化する場合はデータの連結・変換が必要
- 動画データ
    - メリット: 動画データとして保存が可能
    - デメリット: データの加工が困難・データ通信量は画像より多い

今回はエッジデバイスはスペックが低いこと、取得データに対して加工を行うことを想定し、RAWデータの内容はコマ撮り画像を用いることにしました。通信プロトコルについて、画像を送信するにあたり次の選択肢を考えました。

プロトコル | 説明
:-|:-
HTTP |一般的な通信プロトコル。同期処理・片方向通信。<br>クライアントのリクエストによりセッションの確立を行う。
FTP|ファイル転送プロトコル。同期処理・片方向通信。<br>2ポート使用するため、HTTPに比べ大きなファイル転送などに最適
AMQP|メッセージングプロトコル。非同期処理・双方向通信。<br>Producer(送信者)-MessageQue(キュー)-Consumer(受信者)間でセッションを確立。<br>高性能・高信頼。Openstackなどで利用されている。
MQTT|メッセージングプロトコル。非同期処理・双方向通信。<br>Client(受信者)-Broker(仲介)-Publisher(送信者)間でセッションを確立。<br>プロトコル自体が軽量で非常にオーバーヘッドが少ない、テキストデータしか送れない。

本開発ではカメラデータの送受信はMQTTが妥当だと判断しました。理由は以下です。  
- 同期処理では一つの通信が阻害された場合、他の通信が滞る可能性がある
    - エッジデバイスは低性能なため、ネットワーク障害は十分にあり得る
- データの送信機会が多く、1オペレーションのオーバーヘッドが少ない方がいい
    - 常にデータを送信し続けることがわかっているため、コネクションを切断する必要がない。

今回は上記を考慮し、MQTTを採用します。

## ストリーミングサーバ→クライアントの通信について

クライアントがストリーミングサーバに対して要求する要件は次の通りです。  
- Webブラウザ上で視聴可能であること
- 低遅延であること
- 低負荷であること

以上から、主要な動画の配信プロトコルの選択を行います。

プロトコル | 説明
-|-|-
RTP/RSTP|動画配信などに主に用いられていた。flashベースなためサポート終了
MPEG-DASH|ISO国際標準規格、ベンダーに依存しないが、iOSでは非対応とのこと
HLS|Apple社が開発している。IETFで承認され、現在youtubeなどの配信にも使われている

今回は広く使用されていることからHLSを選択しました。
-->
<!-- ## HLSの仕組み  

HLSは、チャンクファイルという細切れで分断された動画ファイルと、チャンクファイルのリストで構成されます。  
クライアントはチャンクファイルのリストを参照し動画を読み込みます。  

このチャンクファイルのリストがHTML5クライアントにより随時更新され、新しいチャンクファイルを次々に参照することで、ライブストリーミングを実装しています。

細かな仕様については以下のブログが非常に参考になりました。  
https://did2memo.net/2017/02/20/http-live-streaming/   -->

# アプリケーションの開発

本システムの開発について考えていきます。  

## データの流れ方 

![](/images/2021-10-12-r01/streaming.png)  

ffmpegではHLSのリストの作成まで可能な変換機能はあるのですが、inputのデータがデバイスソケット等ではないため、今回は連番画像から動画を作成する部分だけをffmpegで、他の箇所についてはエントリーポイントとして起動しているプロセス（シェルスクリプト）で対処します。

## ファイル管理


# 今後の改善  

- 1分ほど遅延する
    - HLSの仕組み上、完全にリアルタイムは不可


# 備考  

<!-- MQTTの受信をHTTPサーバと分断して考えてた際は、動画データの配信も考えていました。 -->

<!-- ## 動画データを配信する方法について -->



# 参考  

https://www.dpsj.co.jp/tech-articles/wowza-blog-hls  
https://qiita.com/wktq/items/a6e169e85a8a75c8524f  
http://happyking.blog47.fc2.com/blog-entry-58.html  
https://www.ydc.co.jp/column/0002/20170630.html  
https://tech-lab.sios.jp/archives/7902  
https://www.slideshare.net/terurou/mqttamqpnet
https://qiita.com/yuba/items/00fc1892b296fb7b8de9
https://qiita.com/chihiro/items/9d280704c6eff8603389

[^1] opencvのライブラリはpython 3.10.X非対応
